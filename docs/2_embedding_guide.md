# ğŸ“Š Embedding & VektÃ¶rleÅŸtirme Rehberi

**GÃ¶revliler**: Mehmet, Hasan

## ğŸ¯ AmaÃ§

Bu rehber, LangChain HuggingFaceEmbeddings kullanarak metin parÃ§alarÄ±nÄ± (chunks) anlamsal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmeyi anlatÄ±r. Cosmos-E5-Large modeliyle yÃ¼ksek kalitede embedding oluÅŸturacaksÄ±nÄ±z.

## ğŸ“š Teori & Konsept

### Embedding Nedir?
Embedding, metni sayÄ±sal bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemidir. Bu sayede:
- âœ… Benzer metinler birbirine yakÄ±n vektÃ¶rler oluÅŸturur
- âœ… Anlamsal benzerlik matematiksel olarak hesaplanabilir
- âœ… HÄ±zlÄ± arama yapÄ±labilir

### Cosmos-E5-Large Modeli
- **Boyut**: 1024-dimensional vektÃ¶r
- **Dil**: TÃ¼rkÃ§e ve Ä°ngilizce destekli
- **Performans**: YÃ¼ksek kalite, orta hÄ±z
- **Model ID**: `ytu-ce-cosmos/turkish-e5-large`

## ğŸ”§ Kurulum

### AdÄ±m 1: Gerekli KÃ¼tÃ¼phaneleri YÃ¼kleyin

```bash
pip install langchain
pip install langchain-huggingface
pip install torch
pip install sentence-transformers
```

### AdÄ±m 2: GPU/CPU Kontrol

```python
import torch
print(f"GPU KullanÄ±labilir: {torch.cuda.is_available()}")
```

## ğŸ’¡ Kod Ã–rnekleri

### ADIM 1: Modeli YÃ¼kleyin

```python
import torch
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="ytu-ce-cosmos/turkish-e5-large",
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
print("âœ… Model yÃ¼klendi!")
```

### ADIM 2: Tek Metni VektÃ¶rle

```python
text = "Python programlama dilidir"
embedding = embeddings.embed_query(text)
print(f"Embedding boyutu: {len(embedding)}")  # 1024
```

### ADIM 3: Chunks VektÃ¶rle

```python
from langchain.schema import Document

chunks = [
    Document(page_content="Python programlama dilidir", metadata={"source": "ders1.pdf"}),
    Document(page_content="Makine Ã¶ÄŸrenmesi yapay zekadÄ±r", metadata={"source": "ders2.pdf"}),
]

embeddings_dict = {}
for i, chunk in enumerate(chunks):
    embedding = embeddings.embed_query(chunk.page_content)
    embeddings_dict[i] = {
        "content": chunk.page_content,
        "metadata": chunk.metadata,
        "embedding": embedding
    }
print(f"âœ… {len(embeddings_dict)} chunk vektÃ¶rleÅŸtirildi!")
```

### ADIM 4: Toplu Ä°ÅŸlem (Batch Processing)

```python
texts = ["Metin 1", "Metin 2", "Metin 3"] * 100

# embed_documents() - toplu metinler iÃ§in
embeddings_list = embeddings.embed_documents(texts)
print(f"âœ… {len(embeddings_list)} metin vektÃ¶rleÅŸtirildi!")
```

### ADIM 5: Benzerlik Hesapla

```python
import numpy as np

emb1 = embeddings.embed_query("Makine Ã¶ÄŸrenmesi yapay zekadÄ±r")
emb2 = embeddings.embed_query("Yapay zeka makine Ã¶ÄŸrenmesidir")

similarity = np.dot(emb1, emb2)
print(f"Benzerlik: {similarity:.4f}")
```

## ğŸ“ GerÃ§ek Proje

```python
import pickle
import torch
from langchain.schema import Document
from langchain_huggingface import HuggingFaceEmbeddings

class EmbeddingProcessor:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="ytu-ce-cosmos/turkish-e5-large",
            model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
    
    def process_chunks(self, chunks, batch_size=32):
        embeddings_dict = {}
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            texts = [doc.page_content for doc in batch]
            batch_embeddings = self.embeddings.embed_documents(texts)
            for j, embedding in enumerate(batch_embeddings):
                embeddings_dict[i+j] = {
                    "content": batch[j].page_content,
                    "metadata": batch[j].metadata,
                    "embedding": embedding
                }
        return embeddings_dict
    
    def save(self, embeddings_dict, path):
        with open(path, "wb") as f:
            pickle.dump(embeddings_dict, f)
        print(f"âœ… {len(embeddings_dict)} embedding kaydedildi!")

# KullanÄ±m
processor = EmbeddingProcessor()
embeddings_dict = processor.process_chunks(chunks)
processor.save(embeddings_dict, "embeddings.pkl")
```

## âš¡ Performans

| GPU | 1000 Metin | 10000 Metin |
|-----|-----------|------------|
| CPU | 5-10 min | 50-100 min |
| RTX 3090 | 10-15 sn | 1-2 dakika |
| T4 GPU | 20-30 sn | 2-3 dakika |

## ğŸ§ª Test

```python
import numpy as np

# Test 1: Boyut
embedding = embeddings.embed_query("test")
assert len(embedding) == 1024
print("âœ… Test 1: Boyut doÄŸru")

# Test 2: Normalize
magnitude = np.sqrt(np.sum(np.array(embedding) ** 2))
assert abs(magnitude - 1.0) < 0.01
print("âœ… Test 2: Normalize ediliÅŸ doÄŸru")

# Test 3: Benzerlik
emb1 = embeddings.embed_query("Makine Ã¶ÄŸrenmesi")
emb2 = embeddings.embed_query("Makine Ã¶ÄŸrenmesi")
assert np.dot(emb1, emb2) > 0.99
print("âœ… Test 3: Benzerlik doÄŸru")
```

## ğŸ› Sorun Giderme

### CUDA HafÄ±zasÄ±
```python
embeddings = HuggingFaceEmbeddings(
    model_name="ytu-ce-cosmos/turkish-e5-large",
    model_kwargs={"device": "cpu"},  # CPU kullan
    encode_kwargs={"normalize_embeddings": True}
)
```

### YavaÅŸ Ä°ÅŸlem
```python
# embed_documents() kullan (toplu iÅŸlem)
embeddings_list = embeddings.embed_documents(texts)
```

## ğŸ“ Kontrol Listesi

- [ ] LangChain kurulu
- [ ] langchain-huggingface kurulu
- [ ] Model yÃ¼klendi
- [ ] GPU/CPU ayarÄ± doÄŸru
- [ ] embed_query() test edildi
- [ ] embed_documents() test edildi
- [ ] Embedding boyutu 1024
- [ ] Normalization aktif
- [ ] Benzerlik hesabÄ± Ã§alÄ±ÅŸÄ±yor
- [ ] Embedding'ler kaydedildi
