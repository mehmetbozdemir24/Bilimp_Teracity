# Bilimp Terracity - Teknik DokÃ¼mantasyon

## ğŸ“‹ Proje Ã–zeti

Bu proje, Terracity firmasÄ±nÄ±n Bilimp yazÄ±lÄ±mÄ± iÃ§in geliÅŸtirilmiÅŸ AI destekli dokÃ¼man asistanÄ± sistemidir. Sistem dÃ¶rt ana modÃ¼lden oluÅŸmaktadÄ±r ve her modÃ¼l belirli ekip Ã¼yeleri tarafÄ±ndan geliÅŸtirilmiÅŸtir.

## ğŸ—ï¸ Mimari TasarÄ±m

### Genel AkÄ±ÅŸ

```
DokÃ¼manlar (PDF, DOCX, TXT, XLSX)
         â†“
[1. Preprocessing Module] â†’ Chunk'lara ayÄ±rma
         â†“
[2. Embedding Module] â†’ VektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (1024-dim)
         â†“
[3. Database Module] â†’ Qdrant'a yÃ¼kleme
         â†“
[4. LLM Module] â†’ Sorgulama & YanÄ±t Ã¼retimi
```

## ğŸ“¦ ModÃ¼l DetaylarÄ±

### 1. Document Preprocessing Module
**Dosya**: `src/preprocessing/document_processor.py`  
**Sorumlular**: Engin, Batuhan

**Ã–zellikler**:
- Ã‡oklu format desteÄŸi (PDF, DOCX, TXT, XLSX)
- LangChain tabanlÄ± dokÃ¼man yÃ¼kleme
- RecursiveCharacterTextSplitter ile akÄ±llÄ± chunking
- Ã–zelleÅŸtirilebilir chunk boyutu ve overlap
- Metadata yÃ¶netimi

**Temel SÄ±nÄ±flar**:
```python
class DocumentProcessor:
    def __init__(self, chunk_size=512, chunk_overlap=50)
    def load_document(self, file_path) -> List[Dict]
    def chunk_documents(self, documents) -> List[Dict]
    def process_directory(self, directory_path) -> List[Dict]
```

**KullanÄ±m**:
```python
processor = DocumentProcessor(chunk_size=512, chunk_overlap=50)
chunks = processor.process_directory("data/raw")
```

### 2. Embedding Generation Module
**Dosya**: `src/embeddings/embedding_generator.py`  
**Sorumlular**: Mehmet, Hasan

**Ã–zellikler**:
- Multilingual-e5-large model (Cosmos-e5-large olarak referans edilir)
- 1024 boyutlu embedding vektÃ¶rleri
- Batch processing ile yÃ¼ksek performans
- GPU/CPU otomatik algÄ±lama
- Query ve passage embedding ayrÄ±mÄ±

**Temel SÄ±nÄ±flar**:
```python
class EmbeddingGenerator:
    def __init__(self, model_name="intfloat/multilingual-e5-large", batch_size=32)
    def generate_embedding(self, text) -> np.ndarray
    def generate_embeddings_batch(self, texts) -> List[np.ndarray]
    def process_chunks(self, chunks) -> List[Dict]
    def generate_query_embedding(self, query) -> np.ndarray
```

**KullanÄ±m**:
```python
generator = EmbeddingGenerator()
chunks_with_embeddings = generator.process_chunks(chunks)
```

**Model DetaylarÄ±**:
- Model: intfloat/multilingual-e5-large
- Embedding Boyutu: 1024
- Dil DesteÄŸi: Ã‡oklu dil (TÃ¼rkÃ§e dahil)
- Prefix: "passage:" (dokÃ¼manlar iÃ§in), "query:" (sorgular iÃ§in)

### 3. Qdrant Database Module
**Dosya**: `src/database/qdrant_client.py`  
**Sorumlular**: SÃ¼leyman, Eren

**Ã–zellikler**:
- Qdrant vektÃ¶r veritabanÄ± entegrasyonu
- Otomatik collection yÃ¶netimi
- Batch upload desteÄŸi
- Cosine similarity search
- Metadata filtreleme
- Collection bilgi sorgulama

**Temel SÄ±nÄ±flar**:
```python
class QdrantDatabase:
    def __init__(self, host="localhost", port=6333, collection_name="bilimp_documents", embedding_dim=1024)
    def upload_chunks(self, chunks, batch_size=100) -> int
    def search(self, query_embedding, limit=5, score_threshold=None, filters=None) -> List[Dict]
    def get_collection_info() -> Dict
    def delete_collection()
```

**KullanÄ±m**:
```python
db = QdrantDatabase(host="localhost", port=6333)
db.upload_chunks(chunks_with_embeddings)
results = db.search(query_embedding, limit=5)
```

**VeritabanÄ± YapÄ±sÄ±**:
- Distance Metric: Cosine Similarity
- Vector Size: 1024
- Payload: text (string), metadata (dict)

### 4. LLM Integration Module
**Dosya**: `src/llm/llm_client.py`  
**Sorumlular**: Hasan, Eren

**Ã–zellikler**:
- Ollama API entegrasyonu
- Ã‡oklu model desteÄŸi (Gemma3-12B, Qwen3-9B)
- RAG (Retrieval-Augmented Generation) pipeline
- Context-aware response generation
- TÃ¼rkÃ§e prompt optimizasyonu

**Temel SÄ±nÄ±flar**:
```python
class LLMClient:
    def __init__(self, base_url="http://localhost:11434", gemma_model="gemma3:12b", qwen_model="qwen3:9b")
    def generate_response(self, query, context, model=None) -> str
    def generate_with_gemma(self, query, context) -> str
    def generate_with_qwen(self, query, context) -> str
    def generate_with_both_models(self, query, context) -> Dict[str, str]

class RAGSystem:
    def __init__(self, database, embedding_generator, llm_client, top_k=5)
    def query(self, query, model=None, temperature=0.7) -> Dict
```

**KullanÄ±m**:
```python
llm_client = LLMClient()
rag_system = RAGSystem(db, generator, llm_client, top_k=5)
result = rag_system.query("Bilimp nedir?")
```

**RAG Pipeline**:
1. Query embedding Ã¼retimi
2. Qdrant'tan benzer dokÃ¼manlarÄ± getirme
3. Context oluÅŸturma
4. LLM ile yanÄ±t Ã¼retimi

## ğŸ³ Docker AltyapÄ±sÄ±

### docker-compose.yml

Ä°ki temel servis:

1. **Qdrant**: VektÃ¶r veritabanÄ±
   - Port: 6333 (REST), 6334 (gRPC)
   - Volume: ./qdrant_storage
   - Otomatik restart

2. **Ollama**: LLM runtime
   - Port: 11434
   - Volume: ./ollama_models
   - GPU desteÄŸi
   - Otomatik restart

### Servis YÃ¶netimi

```bash
# BaÅŸlat
docker-compose up -d

# Durdur
docker-compose down

# LoglarÄ± gÃ¶rÃ¼ntÃ¼le
docker-compose logs -f

# Servis durumu
docker-compose ps
```

## ğŸ”„ Main Pipeline

**Dosya**: `main.py`

Ana orchestration sÄ±nÄ±fÄ± tÃ¼m modÃ¼lleri koordine eder.

### BilimpPipeline SÄ±nÄ±fÄ±

```python
class BilimpPipeline:
    def initialize_components()           # TÃ¼m modÃ¼lleri baÅŸlat
    def process_documents(input_dir)      # DokÃ¼manlarÄ± iÅŸle
    def generate_embeddings(chunks)       # Embedding Ã¼ret
    def upload_to_database(chunks)        # Qdrant'a yÃ¼kle
    def run_full_pipeline(input_dir)      # TÃ¼m pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
    def query_system(query, model)        # Sistemi sorgula
```

### KullanÄ±m ModlarÄ±

1. **Process Mode**: DokÃ¼manlarÄ± iÅŸle ve yÃ¼kle
```bash
python main.py --mode process --input-dir data/raw
```

2. **Query Mode**: Sistemi sorgula
```bash
python main.py --mode query --query "Soru" --model gemma
```

3. **Info Mode**: Sistem bilgisi
```bash
python main.py --mode info
```

## âš™ï¸ KonfigÃ¼rasyon

### Ortam DeÄŸiÅŸkenleri (.env)

```env
# Embedding AyarlarÄ±
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_DIMENSION=1024
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Qdrant AyarlarÄ±
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=bilimp_documents

# LLM AyarlarÄ±
GEMMA_MODEL=gemma3:12b
QWEN_MODEL=qwen3:9b
OLLAMA_BASE_URL=http://localhost:11434

# Ä°ÅŸlem AyarlarÄ±
MAX_WORKERS=4
BATCH_SIZE=32
```

### VarsayÄ±lan DeÄŸerler

EÄŸer .env dosyasÄ± yoksa veya bir deÄŸiÅŸken tanÄ±mlÄ± deÄŸilse, kod iÃ§inde varsayÄ±lan deÄŸerler kullanÄ±lÄ±r.

## ğŸ“Š Performans Metrikleri

### Beklenen Performans

- **DokÃ¼man Ä°ÅŸleme**: ~50-100 sayfa/saniye
- **Embedding Ãœretimi**: 
  - GPU: ~100 chunk/saniye
  - CPU: ~10-20 chunk/saniye
- **VektÃ¶r Upload**: ~1000 chunk/saniye
- **Similarity Search**: <100ms (1M vektÃ¶r)
- **LLM Response**: 2-5 saniye

### Sistem Gereksinimleri

**Minimum**:
- CPU: 4 cores
- RAM: 8GB
- Disk: 20GB

**Ã–nerilen**:
- CPU: 8+ cores
- RAM: 16GB+
- GPU: NVIDIA 8GB+ VRAM
- Disk: 50GB+ SSD

## ğŸ” GÃ¼venlik NotlarÄ±

1. `.env` dosyasÄ± git'e commit edilmemeli
2. Ãœretim ortamÄ±nda Qdrant authentication aktif edilmeli
3. Ollama sadece gÃ¼venilir aÄŸlardan eriÅŸilebilir olmalÄ±
4. API rate limiting uygulanmalÄ±

## ğŸ§ª Test Stratejisi

### Birim Testleri
Her modÃ¼l iÃ§in ayrÄ± test dosyalarÄ± oluÅŸturulabilir:
- `tests/test_preprocessing.py`
- `tests/test_embeddings.py`
- `tests/test_database.py`
- `tests/test_llm.py`

### Entegrasyon Testi
`example_usage.py` dosyasÄ± tam pipeline'Ä± test eder.

### Test Komutu
```bash
python example_usage.py
```

## ğŸ“ˆ Ã–lÃ§eklendirme

### Horizontal Scaling
- Qdrant cluster kurulumu
- Multiple Ollama instances
- Load balancing

### Vertical Scaling
- GPU upgrade
- RAM artÄ±rÄ±mÄ±
- Batch size optimizasyonu

## ğŸ”§ BakÄ±m ve Ä°zleme

### Log YÃ¶netimi
TÃ¼m modÃ¼ller Python logging kullanÄ±r:
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Monitoring
- Qdrant metrics: http://localhost:6333/metrics
- Docker stats: `docker stats`
- Application logs: Python logging output

## ğŸ“š BaÄŸÄ±mlÄ±lÄ±klar

### Ana KÃ¼tÃ¼phaneler
- **langchain**: Document loading ve processing
- **sentence-transformers**: Embedding generation
- **qdrant-client**: Vector database
- **requests**: HTTP communication
- **torch**: Deep learning backend

### Tam Liste
TÃ¼m baÄŸÄ±mlÄ±lÄ±klar `requirements.txt` dosyasÄ±nda.

## ğŸš€ Deployment

### Development
```bash
./setup.sh
python example_usage.py
```

### Production
1. Environment variables ayarla
2. Docker compose ile servisleri baÅŸlat
3. Systemd service oluÅŸtur
4. Nginx reverse proxy
5. SSL/TLS yapÄ±landÄ±r

## ğŸ“ Versiyon GeÃ§miÅŸi

- **v1.0.0** (2024-10-24): Ä°lk release
  - TÃ¼m temel modÃ¼ller implementasyonu
  - Docker altyapÄ±sÄ±
  - DokÃ¼mantasyon

## ğŸ‘¥ KatkÄ±da Bulunanlar

| ModÃ¼l | Sorumlular |
|-------|------------|
| Document Processing | Engin, Batuhan |
| Embedding Generation | Mehmet, Hasan |
| Database Management | SÃ¼leyman, Eren |
| LLM Integration | Hasan, Eren |

## ğŸ¯ Gelecek GeliÅŸtirmeler

- [ ] Web UI eklenmesi
- [ ] REST API endpoint'leri
- [ ] Ã‡oklu dil desteÄŸi geniÅŸletme
- [ ] Streaming response support
- [ ] Fine-tuning capabilities
- [ ] Advanced analytics dashboard
- [ ] User authentication system
- [ ] Document update/delete operations
- [ ] Incremental indexing
- [ ] A/B testing framework

## ğŸ“ Destek

Teknik destek iÃ§in:
- GitHub Issues: [Bilimp_Terracity/issues](https://github.com/mehmetbozdemir24/Bilimp_Terracity/issues)
- Repository: [Bilimp_Terracity](https://github.com/mehmetbozdemir24/Bilimp_Terracity)
