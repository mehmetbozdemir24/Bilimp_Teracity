# Bilimp Terracity Asistan

Terracity FirmasÄ±na ait Bilimp YazÄ±lÄ±mÄ± iÃ§in geliÅŸtirilmiÅŸ AI Asistan Sistemi. Bu sistem, dokÃ¼manlarÄ±n iÅŸlenmesi, embedding vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi ve semantik arama/yanÄ±t Ã¼retimi iÃ§in tasarlanmÄ±ÅŸtÄ±r.

## ğŸ¯ Proje Ã–zellikleri

### 1. DokÃ¼man Ä°ÅŸleme ve Chunking (Engin, Batuhan)
- PDF, DOCX, TXT, XLSX formatlarÄ±nda dokÃ¼man desteÄŸi
- AkÄ±llÄ± metin bÃ¶lÃ¼mleme (chunking) algoritmasÄ±
- Metadata yÃ¶netimi ve kaynak takibi

### 2. Embedding Ãœretimi (Mehmet, Hasan)
- **Cosmos-e5-large** (multilingual-e5-large) modeli ile embedding Ã¼retimi
- Batch processing desteÄŸi
- GPU acceleration
- 1024 boyutlu vektÃ¶r desteÄŸi

### 3. VektÃ¶r VeritabanÄ± YÃ¶netimi (SÃ¼leyman, Eren)
- **Qdrant** vektÃ¶r veritabanÄ± entegrasyonu
- Docker ile kolay kurulum
- YÃ¼ksek performanslÄ± cosine similarity arama
- Metadata filtreleme

### 4. LLM TabanlÄ± YanÄ±t Ãœretimi (Hasan, Eren)
- **Gemma3-12B** ve **Qwen3-9B** model desteÄŸi
- RAG (Retrieval-Augmented Generation) mimarisi
- TÃ¼rkÃ§e dil desteÄŸi
- Ollama ile local deployment

## ğŸ“‹ Gereksinimler

- Python 3.8+
- Docker ve Docker Compose
- CUDA destekli GPU (opsiyonel, performans iÃ§in Ã¶nerilir)
- En az 16GB RAM
- Ollama (LLM Ã§alÄ±ÅŸtÄ±rma iÃ§in)

## ğŸš€ Kurulum

### 1. Repository'yi KlonlayÄ±n

```bash
git clone https://github.com/mehmetbozdemir24/Bilimp_Terracity.git
cd Bilimp_Terracity
```

### 2. Python Sanal OrtamÄ± OluÅŸturun

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate  # Windows
```

### 3. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin

```bash
pip install -r requirements.txt
```

### 4. Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n

```bash
cp .env.example .env
# .env dosyasÄ±nÄ± dÃ¼zenleyin
```

### 5. Docker Servislerini BaÅŸlatÄ±n

```bash
# Qdrant ve Ollama servislerini baÅŸlat
docker-compose up -d
```

### 6. Ollama Modellerini YÃ¼kleyin

```bash
# Gemma3-12B modelini yÃ¼kle
docker exec -it bilimp_ollama ollama pull gemma3:12b

# Qwen3-9B modelini yÃ¼kle
docker exec -it bilimp_ollama ollama pull qwen3:9b
```

## ğŸ’» KullanÄ±m

### DokÃ¼manlarÄ± Ä°ÅŸleme

1. DokÃ¼manlarÄ±nÄ±zÄ± `data/raw/` klasÃ¶rÃ¼ne yerleÅŸtirin
2. Pipeline'Ä± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python main.py --mode process --input-dir data/raw
```

Bu iÅŸlem:
- DokÃ¼manlarÄ± okuyup chunk'lara ayÄ±rÄ±r
- Her chunk iÃ§in embedding Ã¼retir
- Embedding'leri Qdrant'a yÃ¼kler

### Sistemi Sorgulama

```bash
# Gemma3 modeli ile sorgulama
python main.py --mode query --query "Bilimp sistemi nedir?" --model gemma

# Qwen3 modeli ile sorgulama
python main.py --mode query --query "Bilimp sistemi nedir?" --model qwen
```

### Sistem Bilgisi

```bash
python main.py --mode info
```

## ğŸ“ Proje YapÄ±sÄ±

```
Bilimp_Terracity/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/        # DokÃ¼man iÅŸleme modÃ¼lÃ¼ (Engin, Batuhan)
â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â”œâ”€â”€ embeddings/          # Embedding Ã¼retimi (Mehmet, Hasan)
â”‚   â”‚   â””â”€â”€ embedding_generator.py
â”‚   â”œâ”€â”€ database/            # Qdrant entegrasyonu (SÃ¼leyman, Eren)
â”‚   â”‚   â””â”€â”€ qdrant_client.py
â”‚   â””â”€â”€ llm/                 # LLM entegrasyonu (Hasan, Eren)
â”‚       â””â”€â”€ llm_client.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Ham dokÃ¼manlar
â”‚   â””â”€â”€ processed/           # Ä°ÅŸlenmiÅŸ veri
â”œâ”€â”€ configs/                 # KonfigÃ¼rasyon dosyalarÄ±
â”œâ”€â”€ main.py                  # Ana pipeline
â”œâ”€â”€ docker-compose.yml       # Docker servisleri
â”œâ”€â”€ requirements.txt         # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â””â”€â”€ .env.example            # Ã–rnek ortam deÄŸiÅŸkenleri
```

## ğŸ”§ KonfigÃ¼rasyon

`.env` dosyasÄ±ndaki Ã¶nemli parametreler:

```env
# Embedding AyarlarÄ±
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_DIMENSION=1024
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Qdrant AyarlarÄ±
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=bilimp_documents

# LLM AyarlarÄ±
GEMMA_MODEL=gemma3:12b
QWEN_MODEL=qwen3:9b
OLLAMA_BASE_URL=http://localhost:11434
```

## ğŸ§ª Test

Sistemi test etmek iÃ§in Ã¶rnek bir dokÃ¼man ekleyin:

```bash
# Ã–rnek metin dosyasÄ± oluÅŸtur
echo "Bilimp sistemi dokÃ¼man yÃ¶netimi iÃ§in kullanÄ±lÄ±r." > data/raw/test.txt

# Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
python main.py --mode process --input-dir data/raw

# Sistemi sorgula
python main.py --mode query --query "Bilimp nedir?"
```

## ğŸ” ModÃ¼l DetaylarÄ±

### 1. Document Processor
- **Sorumlular**: Engin, Batuhan
- Ã‡eÅŸitli formatlarda dokÃ¼man okuma
- RecursiveCharacterTextSplitter ile akÄ±llÄ± chunking
- Metadata yÃ¶netimi

### 2. Embedding Generator
- **Sorumlular**: Mehmet, Hasan
- Multilingual-e5-large model
- Batch processing ile yÃ¼ksek performans
- Query ve passage embedding'leri

### 3. Qdrant Client
- **Sorumlular**: SÃ¼leyman, Eren
- Collection yÃ¶netimi
- Batch upload
- Similarity search
- Metadata filtreleme

### 4. LLM Client
- **Sorumlular**: Hasan, Eren
- Ollama API entegrasyonu
- Ã‡oklu model desteÄŸi (Gemma3, Qwen3)
- RAG pipeline
- Context-aware response generation

## ğŸ“Š Performans

- **Embedding Ãœretimi**: ~100 chunks/saniye (GPU ile)
- **VektÃ¶r Arama**: <100ms (1M vektÃ¶r iÃ§in)
- **Response Generation**: 2-5 saniye (model boyutuna baÄŸlÄ±)

## ğŸ› ï¸ GeliÅŸtirme

### Yeni DokÃ¼man FormatÄ± Ekleme

`src/preprocessing/document_processor.py` iÃ§inde `loaders` dictionary'sine yeni format ekleyin:

```python
self.loaders = {
    '.pdf': PyPDFLoader,
    '.yeni_format': YeniFormatLoader,  # Yeni format
    ...
}
```

### Yeni LLM Modeli Ekleme

Ollama ile yeni model ekleyin:

```bash
docker exec -it bilimp_ollama ollama pull yeni_model:tag
```

## ğŸ› Sorun Giderme

### Qdrant BaÄŸlantÄ± HatasÄ±
```bash
# Qdrant servisini kontrol edin
docker ps | grep qdrant
docker logs bilimp_qdrant
```

### Ollama Model BulunamadÄ±
```bash
# Mevcut modelleri listeleyin
docker exec -it bilimp_ollama ollama list

# Model yÃ¼kleyin
docker exec -it bilimp_ollama ollama pull gemma3:12b
```

### GPU KullanÄ±lamÄ±yor
```bash
# NVIDIA Docker runtime'Ä± kontrol edin
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

## ğŸ“ Lisans

Bu proje Terracity firmasÄ± iÃ§in Ã¶zel olarak geliÅŸtirilmiÅŸtir.

## ğŸ‘¥ Ekip

- **DokÃ¼man Ä°ÅŸleme**: Engin, Batuhan
- **Embedding Ãœretimi**: Mehmet, Hasan
- **VeritabanÄ± YÃ¶netimi**: SÃ¼leyman, Eren
- **LLM Entegrasyonu**: Hasan, Eren

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in: [mehmetbozdemir24](https://github.com/mehmetbozdemir24)
